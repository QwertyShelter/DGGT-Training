{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c6fa514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import torch\n",
    "import lpips\n",
    "import numpy as np\n",
    "from dggt.models.vggt import VGGT\n",
    "from dggt.utils.pose_enc import pose_encoding_to_extri_intri\n",
    "from dggt.utils.geometry import unproject_depth_map_to_point_map\n",
    "from dggt.utils.gs import concat_list, get_split_gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da2cc2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_t(t, t0, alpha, gamma0 = 1, gamma1 = 0.1):\n",
    "    sigma = torch.log(torch.tensor(gamma1)).to(gamma0.device) / ((gamma0)**2 + 1e-6)\n",
    "    conf = torch.exp(sigma*torch.tensor(t0-t).to(gamma0.device)**2)\n",
    "    alpha_ = alpha * conf\n",
    "    return alpha_.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "535b59e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames_from_video(video_path, max_frames=100):\n",
    "    \"\"\"Extract frames from video\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    interval = max(1, total_frames // max_frames)\n",
    "    \n",
    "    frame_count = 0\n",
    "    while cap.isOpened() and len(frames) < max_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        if frame_count % interval == 0:\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(frame_rgb)\n",
    "        \n",
    "        frame_count += 1\n",
    "    \n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "def save_ply(filename, pts3d, view):\n",
    "    # ðŸ§¹ è¿‡æ»¤éžæ³•æ•°å€¼\n",
    "    valid_mask = np.isfinite(pts3d).all(axis=1)\n",
    "    valid_mask &= np.isfinite(view).all(axis=1)\n",
    "    pts3d = pts3d[valid_mask]\n",
    "    view = view[valid_mask]\n",
    "\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write('ply\\nformat ascii 1.0\\n')\n",
    "        f.write(f'element vertex {len(pts3d)}\\n')\n",
    "        f.write('property float x\\nproperty float y\\nproperty float z\\n')\n",
    "        f.write('property uchar red\\nproperty uchar green\\nproperty uchar blue\\n')\n",
    "        f.write('end_header\\n')\n",
    "        for i, p in enumerate(pts3d):\n",
    "            c = view[i]\n",
    "            f.write(f'{p[0]} {p[1]} {p[2]} {c[0]} {c[1]} {c[2]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "254cd93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"davis/bear\"\n",
    "input_views = 1\n",
    "ckpt_path = \"checkpoints/model_latest_waymo.pt\"\n",
    "output_path = \"result/test/\"\n",
    "output_images = True\n",
    "output_depth = True\n",
    "output_metrics = True\n",
    "\n",
    "device = \"cuda:2\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a36236fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.dataset import load_and_preprocess_images\n",
    "\n",
    "image_path_list = []\n",
    "\n",
    "for root, dirs, files in os.walk(image_dir):\n",
    "    for file in sorted(files):\n",
    "        file = os.path.join(root, file)\n",
    "        image_path_list.append(file)\n",
    "\n",
    "images = load_and_preprocess_images(image_path_list)    # [T, C, H, W]\n",
    "\n",
    "bg_mask = np.ones((1, images.shape[0], images.shape[2], images.shape[3]), dtype=bool)   # [B, T, H, W]\n",
    "bg_mask = torch.from_numpy(bg_mask).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "864eed3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangpeifeng/miniconda3/envs/dggt/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/wangpeifeng/miniconda3/envs/dggt/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /home/wangpeifeng/miniconda3/envs/dggt/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangpeifeng/miniconda3/envs/dggt/lib/python3.10/site-packages/lpips/lpips.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(model_path, map_location='cpu'), strict=False)\n",
      "/tmp/ipykernel_816569/4188562551.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float32\n",
    "loss_fn = lpips.LPIPS(net='alex').to(device)\n",
    "\n",
    "# åŠ è½½æ¨¡åž‹ DGGT\n",
    "model = VGGT().to(device)\n",
    "checkpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "model.load_state_dict(checkpoint, strict=True)\n",
    "\n",
    "model.eval()\n",
    "psnr_list, ssim_list, lpips_list = [], [], []\n",
    "inference_time_list = []\n",
    "scene_idx = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a84cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/raid5/wangpeifeng/project/4DReconstruction/dggt/dggt/models/vggt.py:61: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "pts3d_all, rgbs_all = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    with torch.amp.autocast(dtype=dtype, device_type=device):\n",
    "        predictions = model(images.to(device))              # B, T, C, H, W\n",
    "        T, _, H, W = images.shape\n",
    "        extrinsics, intrinsics = pose_encoding_to_extri_intri(predictions['pose_enc'], (H, W))\n",
    "        extrinsic = extrinsics[0]\n",
    "        bottom = torch.tensor([0.0, 0.0, 0.0, 1.0], device=extrinsic.device).view(1, 1, 4).expand(extrinsic.shape[0], 1, 4)\n",
    "        extrinsic = torch.cat([extrinsic, bottom], dim=1)\n",
    "        intrinsic = intrinsics[0]\n",
    "        views=input_views\n",
    "\n",
    "        use_depth = True\n",
    "        if use_depth:\n",
    "            depth_map = predictions[\"depth\"][0]\n",
    "            point_map = unproject_depth_map_to_point_map(depth_map, extrinsics[0], intrinsics[0])[None,...]\n",
    "            point_map = torch.from_numpy(point_map).to(device).float()\n",
    "        else:\n",
    "            point_map = predictions[\"world_points\"]\n",
    "        gs_map = predictions[\"gs_map\"]\n",
    "        gs_conf = predictions[\"gs_conf\"]\n",
    "        dy_map = predictions[\"dynamic_conf\"].squeeze(-1)    # B, T, H, W\n",
    "\n",
    "        # static prediction\n",
    "        static_mask = (dy_map < 0.5)\n",
    "        static_points = point_map[static_mask].reshape(-1, 3)\n",
    "        gs_dynamic_list = dy_map[static_mask].sigmoid()\n",
    "        static_rgbs, static_opacity, static_scales, static_rotations = get_split_gs(gs_map, static_mask)\n",
    "        static_opacity = static_opacity * (1 - gs_dynamic_list)\n",
    "        static_gs_conf = gs_conf[static_mask]\n",
    "\n",
    "        # è®©æ¯ä¸ªæœ‰æ•ˆåƒç´ ç‚¹éƒ½èƒ½å¤Ÿå¯¹åº”åˆ°æ—¶é—´æˆ³ä¸Š\n",
    "        frame_idx = torch.nonzero(static_mask, as_tuple=False)[:, 1].cpu().numpy()  # B, T, H, W -> N, 4 -> N,  (å–ç¬¬äºŒç»´åº¦ T)\n",
    "        \n",
    "        start_idx = 0\n",
    "        indices = [start_idx + i for i in range(T)]\n",
    "        \n",
    "        timestamps = np.array(indices) - start_idx\n",
    "        timestamps = timestamps / timestamps[-1] * (T / 4)\n",
    "        \n",
    "        gs_timestamps = timestamps[frame_idx]\n",
    "        \n",
    "        # dynamic prediction\n",
    "        dynamic_points, dynamic_rgbs, dynamic_opacitys, dynamic_scales, dynamic_rotations = [], [], [], [], []\n",
    "        for i in range(dy_map.shape[1]):                    # shape[1] for B, T, C, H, W\n",
    "            point_map_i = point_map[:, i]                   # B, T, H, W, 3 -> B, H, W, 3\n",
    "            bg_mask_i = bg_mask[:, i]\n",
    "            dy_conf_i = dy_map[:, i].sigmoid()\n",
    "\n",
    "            # points gs_attr ç›´æŽ¥ä½¿ç”¨ bg_mask\n",
    "            dynamic_point = point_map_i[bg_mask_i].reshape(-1, 3)\n",
    "            dynamic_rgb, dynamic_opacity, dynamic_scale, dynamic_rotation = get_split_gs(gs_map[:, i], bg_mask_i)\n",
    "            # åªæœ‰è®¡ç®—ä¸é€æ˜Žåº¦çš„æ—¶å€™æ‰ç”¨åˆ° dy_map\n",
    "            gs_dynamic_list_i = dy_map[:, i][bg_mask_i].sigmoid()\n",
    "            dynamic_opacity = dynamic_opacity * gs_dynamic_list_i\n",
    "\n",
    "            dynamic_points.append(dynamic_point)\n",
    "            dynamic_rgbs.append(dynamic_rgb)\n",
    "            dynamic_opacitys.append(dynamic_opacity)\n",
    "            dynamic_scales.append(dynamic_scale)\n",
    "            dynamic_rotations.append(dynamic_rotation)\n",
    "\n",
    "        chunked_renders, chunked_alphas = [], []  \n",
    "        for idx in range(dy_map.shape[1]):\n",
    "            t0 = timestamps[idx]\n",
    "            static_opacity_ = alpha_t(gs_timestamps, t0, static_opacity, gamma0 = static_gs_conf)\n",
    "            static_gs_list = [static_points, static_rgbs, static_opacity_, static_scales, static_rotations]\n",
    "            '''\n",
    "            if dynamic_points:\n",
    "                world_points, rgbs, opacity, scales, rotation = concat_list(\n",
    "                    static_gs_list,\n",
    "                    [dynamic_points[idx], dynamic_rgbs[idx], dynamic_opacitys[idx], dynamic_scales[idx], dynamic_rotations[idx]]\n",
    "                )\n",
    "            else:\n",
    "                world_points, rgbs, opacity, scales, rotation = static_gs_list\n",
    "            '''\n",
    "            if dynamic_points is None:\n",
    "                continue\n",
    "            \n",
    "            world_points, rgbs, opacity, scales, rotation = static_gs_list\n",
    "            # world_points, rgbs, opacity, scales, rotation = [dynamic_points[idx], dynamic_rgbs[idx], dynamic_opacitys[idx], dynamic_scales[idx], dynamic_rotations[idx]]\n",
    "\n",
    "            # å°† world_points å’Œ rgb æ‹¿å‡ºæ¥å°±å¥½äº†\n",
    "            pts3d_all.append(world_points.cpu().numpy())\n",
    "            rgbs_all.append((rgbs * 255).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8acb209b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "def save_ply_open3d(filename, pts3d, view):\n",
    "    pts = np.asarray(pts3d).reshape(-1,3)\n",
    "    colors = np.asarray(view).reshape(-1, view.shape[-1])\n",
    "    if colors.shape[1] == 4:\n",
    "        colors = colors[:, :3]\n",
    "    # open3d expects colors float in [0,1]\n",
    "    if colors.dtype != np.float32 and colors.dtype != np.float64:\n",
    "        colors = colors.astype(np.float32) / 255.0\n",
    "    else:\n",
    "        if colors.max() > 1.5:\n",
    "            colors = colors / 255.0\n",
    "    pc = o3d.geometry.PointCloud()\n",
    "    pc.points = o3d.utility.Vector3dVector(pts)\n",
    "    pc.colors = o3d.utility.Vector3dVector(colors)\n",
    "    o3d.io.write_point_cloud(filename, pc, write_ascii=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d5c9eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "for idx in range(len(pts3d_all)):\n",
    "    filename = os.path.join(output_path, f'scene_{scene_idx:03d}_frame_{idx:03d}.ply')\n",
    "    # save_ply(filename, pts3d_all[idx].reshape(-1, 3), rgbs_all[idx].reshape(-1, 3))\n",
    "    save_ply_open3d(filename, pts3d_all[idx], rgbs_all[idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dggt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
