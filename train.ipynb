{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08334874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import lpips\n",
    "import random\n",
    "import imageio\n",
    "import argparse\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "\n",
    "from dggt.models.vggt import VGGT\n",
    "from dggt.utils.gs import concat_list, get_split_gs\n",
    "from dggt.utils.pose_enc import pose_encoding_to_extri_intri\n",
    "from dggt.utils.geometry import unproject_depth_map_to_point_map\n",
    "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
    "\n",
    "from gsplat.rendering import rasterization\n",
    "from datasets.dataset import WaymoOpenDataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW\n",
    "import torch.distributed as dist\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "def alpha_t(t, t0, alpha, gamma0 = 1, gamma1 = 0.1):\n",
    "    sigma = torch.log(torch.tensor(gamma1)).to(gamma0.device) / ((gamma0)**2 + 1e-6)\n",
    "    conf = torch.exp(sigma*(t0-t)**2)\n",
    "    alpha_ = alpha * conf\n",
    "    return alpha_.float()\n",
    "\n",
    "def compute_metrics(img1, img2, loss_fn):\n",
    "    img1 = img1.clamp(0, 1)\n",
    "    img2 = img2.clamp(0, 1)\n",
    "    psnr_list, ssim_list, lpips_list = [], [], []\n",
    "    for i in range(img1.shape[0]):\n",
    "        im1 = img1[i].cpu().permute(1, 2, 0).numpy()\n",
    "        im2 = img2[i].cpu().permute(1, 2, 0).numpy()\n",
    "        psnr = peak_signal_noise_ratio(im1, im2, data_range=1.0)\n",
    "        ssim = structural_similarity(im1, im2, channel_axis=2, data_range=1.0)\n",
    "        lpips_val = loss_fn(img1[i].unsqueeze(0) * 2 - 1, img2[i].unsqueeze(0) * 2 - 1)\n",
    "        psnr_list.append(psnr)\n",
    "        ssim_list.append(ssim)\n",
    "        lpips_list.append(lpips_val.item())\n",
    "    return sum(psnr_list) / len(psnr_list), sum(ssim_list) / len(ssim_list), sum(lpips_list) / len(lpips_list)\n",
    "\n",
    "def calculate_scale_factor(P1, P2):\n",
    "    distances_P1 = torch.norm(P1[1:], dim=1)  \n",
    "    distances_P2 = torch.norm(P2[1:], dim=1)  \n",
    "    avg_distance_P1 = torch.mean(distances_P1)\n",
    "    if avg_distance_P1 < 0.1: #almost not move\n",
    "        return 1\n",
    "    avg_distance_P2 = torch.mean(distances_P2)\n",
    "    scale_factor = avg_distance_P2 / avg_distance_P1\n",
    "    return scale_factor\n",
    "\n",
    "def save_video(images, path, fps=8):\n",
    "    images = images.detach().cpu()  # Ensure it's on CPU\n",
    "    if images.max() <= 1.0:\n",
    "        images = images * 255.0\n",
    "    images = images.byte().permute(0, 2, 3, 1).numpy()  # [S, H, W, 3]\n",
    "    \n",
    "    imageio.mimwrite(path, images, fps=fps, codec='libx264')\n",
    "\n",
    "def parse_scene_names(scene_names_str):\n",
    "    scene_names_str = scene_names_str.strip()\n",
    "    if scene_names_str.startswith(\"(\") and scene_names_str.endswith(\")\"):\n",
    "        start, end = scene_names_str[1:-1].split(\",\")\n",
    "        return [str(i).zfill(3) for i in range(int(start), int(end)+1)]\n",
    "    else:\n",
    "        return [str(int(x)).zfill(3) for x in scene_names_str.split()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9459e009",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# TODO: 批量处理？\n",
    "class RGBLoss(nn.Module):\n",
    "    def __init__(self, lambda_lpips=0.1, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.lambda_lpips = lambda_lpips\n",
    "        self.lpips_fn = lpips.LPIPS(net='alex').to(device)\n",
    "        # 设置为评估模式，禁用梯度计算，便于复现\n",
    "        self.lpips_fn.eval()\n",
    "\n",
    "    def forward(self, step, pred, target):\n",
    "        mse_loss = F.l1_loss(pred, target)\n",
    "        \n",
    "        # 修改范围，对于 [0,1] 将映射到 [-1,1]\n",
    "        pred_lpips = pred * 2 - 1 if pred.min() >= 0 else pred\n",
    "        target_lpips = target * 2 - 1 if target.min() >= 0 else target\n",
    "        \n",
    "        lpips_loss = self.lpips_fn(pred_lpips, target_lpips).mean()\n",
    "\n",
    "        # 逐步提高 lpips 损失的权重\n",
    "        lpips_loss = min(step / 1000, 1.0) * lpips_loss\n",
    "        return mse_loss + self.lambda_lpips * lpips_loss\n",
    "    \n",
    "'''\n",
    "for i in range(img1.shape[0]):\n",
    "    im1 = img1[i].cpu().permute(1, 2, 0).numpy()\n",
    "    im2 = img2[i].cpu().permute(1, 2, 0).numpy()\n",
    "    psnr = peak_signal_noise_ratio(im1, im2, data_range=1.0)\n",
    "    ssim = structural_similarity(im1, im2, channel_axis=2, data_range=1.0)\n",
    "    lpips_val = loss_fn(img1[i].unsqueeze(0) * 2 - 1, img2[i].unsqueeze(0) * 2 - 1)\n",
    "'''\n",
    "\n",
    "def compute_lifespan_loss(gamma):\n",
    "    return torch.mean(torch.abs(1 / (gamma + 1e-6)))\n",
    "\n",
    "# FIXME: 如何处理批数据？另外维度可能是 [B, S, D]\n",
    "# TODO: Whether all inputs are not None ?\n",
    "class FeedForwardLoss(nn.Module):\n",
    "    def __init__(self, lambda_o=1, lambda_d=0.05, lambda_l=0.01, lambda_lpips=0.05, device='cpu'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lambda_o, self.lambda_d, self.lambda_l = lambda_o, lambda_d, lambda_l\n",
    "\n",
    "        self.rgb_loss_fn = RGBLoss(lambda_lpips=lambda_lpips, device=device)\n",
    "        self.opacity_loss_fn = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "        self.dymask_loss_fn = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "        self.lifespan_reg_fn = compute_lifespan_loss\n",
    "    \n",
    "    def forward(self, img, target, skymask, t_skymask, dymask, t_dymask, lifespan_params, step):\n",
    "        rgb_loss = self.rgb_loss_fn(step, img, target)\n",
    "        opacity_loss = self.lambda_o * self.opacity_loss_fn(skymask, t_skymask)\n",
    "        if t_dymask is not None:\n",
    "            dymask_loss = self.lambda_d * self.dymask_loss_fn(dymask, t_dymask)\n",
    "        else:\n",
    "            dymask_loss = self.lambda_d * self.dymask_loss_fn(dymask, torch.zeros_like(dymask))\n",
    "        lifespan_loss = self.lambda_l * self.lifespan_reg_fn(lifespan_params)\n",
    "        \n",
    "        total_loss =  rgb_loss + opacity_loss + dymask_loss + lifespan_loss\n",
    "\n",
    "        return {\n",
    "            'total': total_loss,\n",
    "            'rgb': rgb_loss,\n",
    "            'opacity': opacity_loss,\n",
    "            'dymask': dymask_loss,\n",
    "            'lifespan': lifespan_loss\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a77f4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args includes: output_path, log_dir, save_image, save_ckpt, local_rank, max_epoch\n",
    "def train(model, dataloader, optimizer, scheduler, loss_fn, step, dtype, device, args):\n",
    "    \n",
    "    train_loss_list = []\n",
    "    training_time_list = []\n",
    "    # scene_idx = 1\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        # load data from dataloader\n",
    "        images = batch['images'].to(device)\n",
    "        sky_mask = batch['masks'].to(device).permute(0, 1, 3, 4, 2)\n",
    "        bg_mask = (sky_mask == 0).any(dim=-1)\n",
    "        timestamps = batch['timestamps'][0].to(device)\n",
    "\n",
    "        start_time = time.time()\n",
    "        if 'dynamic_mask' in batch:\n",
    "            dynamic_masks = batch['dynamic_mask'].to(device)[:, :, 0, :, :]\n",
    "        else:\n",
    "            dynamic_masks = None\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.amp.autocast(dtype=dtype, device_type=device):\n",
    "            # Get the predictions from the model\n",
    "            predictions = model(images)\n",
    "            H, W = images.shape[-2:]\n",
    "            extrinsics, intrinsics = pose_encoding_to_extri_intri(predictions['pose_enc'], (H, W))\n",
    "            extrinsic = extrinsics[0]\n",
    "            bottom = torch.tensor([0.0, 0.0, 0.0, 1.0], device=extrinsic.device).view(1, 1, 4).expand(extrinsic.shape[0], 1, 4)\n",
    "            extrinsic = torch.cat([extrinsic, bottom], dim=1)\n",
    "            intrinsic = intrinsics[0]\n",
    "\n",
    "            use_depth = True\n",
    "            if use_depth:\n",
    "                depth_map = predictions[\"depth\"][0]\n",
    "                point_map = unproject_depth_map_to_point_map(depth_map, extrinsics[0], intrinsics[0])[None,...]\n",
    "                point_map = torch.from_numpy(point_map).to(device).float()\n",
    "            else:\n",
    "                point_map = predictions[\"world_points\"]\n",
    "            gs_map = predictions[\"gs_map\"]\n",
    "            gs_conf = predictions[\"gs_conf\"]\n",
    "            dy_map = predictions[\"dynamic_conf\"].squeeze(-1) #B,H,W,1\n",
    "\n",
    "            # TODO: 这里训练的时候不需要考虑 dy_map ?\n",
    "            static_mask = (bg_mask & (dy_map < 0.5))\n",
    "            static_points = point_map[static_mask].reshape(-1, 3)\n",
    "            gs_dynamic_list = dy_map[static_mask].sigmoid()\n",
    "            static_rgbs, static_opacity, static_scales, static_rotations = get_split_gs(gs_map, static_mask)\n",
    "            static_opacity = static_opacity * (1 - gs_dynamic_list)\n",
    "            static_gs_conf = gs_conf[static_mask]\n",
    "            frame_idx = torch.nonzero(static_mask, as_tuple=False)[:,1]\n",
    "            gs_timestamps = timestamps[frame_idx]\n",
    "\n",
    "\n",
    "            dynamic_points, dynamic_rgbs, dynamic_opacitys, dynamic_scales, dynamic_rotations = [], [], [], [], []\n",
    "            for i in range(dy_map.shape[1]):\n",
    "                point_map_i = point_map[:, i]\n",
    "                bg_mask_i = bg_mask[:, i]\n",
    "                dynamic_point = point_map_i[bg_mask_i].reshape(-1, 3)\n",
    "                dynamic_rgb, dynamic_opacity, dynamic_scale, dynamic_rotation = get_split_gs(gs_map[:, i], bg_mask_i)\n",
    "                gs_dynamic_list_i = dy_map[:, i][bg_mask_i].sigmoid()\n",
    "                dynamic_opacity = dynamic_opacity * gs_dynamic_list_i\n",
    "                dynamic_points.append(dynamic_point)\n",
    "                dynamic_rgbs.append(dynamic_rgb)\n",
    "                dynamic_opacitys.append(dynamic_opacity)\n",
    "                dynamic_scales.append(dynamic_scale)\n",
    "                dynamic_rotations.append(dynamic_rotation)\n",
    "\n",
    "            chunked_renders, chunked_alphas = [], []\n",
    "            for idx in range(dy_map.shape[1]):\n",
    "                t0 = timestamps[idx]\n",
    "                static_opacity_ = alpha_t(gs_timestamps, t0, static_opacity, gamma0 = static_gs_conf)\n",
    "                static_gs_list = [static_points, static_rgbs, static_opacity_, static_scales, static_rotations]\n",
    "                if dynamic_points:\n",
    "                    world_points, rgbs, opacity, scales, rotation = concat_list(\n",
    "                        static_gs_list,\n",
    "                        [dynamic_points[idx], dynamic_rgbs[idx], dynamic_opacitys[idx], dynamic_scales[idx], dynamic_rotations[idx]]\n",
    "                    )\n",
    "                # TODO: No this branch in train.py\n",
    "                else:\n",
    "                    world_points, rgbs, opacity, scales, rotation = static_gs_list\n",
    "                # P.S. Render_mode in train.py is 'RGB', which means no depth is needed\n",
    "                renders_chunk, alphas_chunk, _ = rasterization(\n",
    "                    means=world_points,\n",
    "                    quats=rotation,\n",
    "                    scales=scales,\n",
    "                    opacities=opacity,\n",
    "                    colors=rgbs,\n",
    "                    viewmats=extrinsic[idx][None],\n",
    "                    Ks=intrinsic[idx][None],\n",
    "                    width=W,\n",
    "                    height=H\n",
    "                )\n",
    "                chunked_renders.append(renders_chunk)\n",
    "                chunked_alphas.append(alphas_chunk)\n",
    "\n",
    "\n",
    "            renders = torch.cat(chunked_renders, dim=0)\n",
    "            alphas = torch.cat(chunked_alphas, dim=0)\n",
    "            bg_render = model.sky_model(images, extrinsic, intrinsic)\n",
    "            bg_render = (bg_render - bg_render.min()) / (bg_render.max() - bg_render.min() + 1e-8)  # TODO: Why this ?\n",
    "            renders = alphas * renders + (1 - alphas) * bg_render\n",
    "            rendered_image = renders.permute(0, 3, 1, 2)\n",
    "            target_image = images[0]\n",
    "\n",
    "            # TODO: scene_name and time\n",
    "            # scene_name = str(scene_idx).zfill(3)\n",
    "            training_time = time.time() - start_time\n",
    "            training_time_list.append(training_time)\n",
    "            \n",
    "            # ============================== Loss ==============================\n",
    "\n",
    "            # 计算全部损失\n",
    "            # FIXME: train.py 中只使用了 static_gs_conf，以及 alpha 和 sky_mask 做 F.l1_loss\n",
    "            loss_dict = loss_fn(rendered_image, target_image, gs_map, bg_mask, dy_map, dynamic_masks, gs_conf, step)\n",
    "        \n",
    "        # 反向传播和优化器更新\n",
    "        loss_dict['total'].backward()           # 反向传播\n",
    "        optimizer.step()                        # 更新参数\n",
    "        scheduler.step()\n",
    "        \n",
    "        # 记录损失\n",
    "        train_loss_list.append(loss_dict['total'].item())\n",
    "        # scene_idx += 1\n",
    "\n",
    "        # =============================== Record ===============================\n",
    "\n",
    "        # TODO: local_rank\n",
    "        if args.local_rank == 0 and step % 1 == 0:\n",
    "            print(f\"[{step}/{args.max_epoch}] Loss: {loss_dict['total'].item():.4f} | LR: {scheduler.get_last_lr()}\")\n",
    "            print(f\"[{step}/{args.max_epoch}]   sky Loss: {loss_dict['opacity'].item():.4f} | LR: {scheduler.get_last_lr()}\")\n",
    "\n",
    "        if args.local_rank == 0 and step % args.save_image == 0:\n",
    "            random_frame_idx = random.randint(0, rendered_image.shape[0] - 1)\n",
    "\n",
    "            rendered = rendered_image[random_frame_idx].detach().cpu().clamp(0, 1)\n",
    "            target = target_image[random_frame_idx].detach().cpu().clamp(0, 1)\n",
    "\n",
    "            dy_map_sigmoid = torch.sigmoid(dy_map[0, random_frame_idx]).detach().cpu()  # shape: (H, W)\n",
    "            dy_map_rgb = dy_map_sigmoid.unsqueeze(0).repeat(3, 1, 1)  # [3, H, W]\n",
    "\n",
    "            sem_rgb = alphas[random_frame_idx, ..., 0].unsqueeze(0).repeat(3, 1, 1).cpu()  # [3, H, W]\n",
    "\n",
    "            combined = torch.cat([target, rendered, dy_map_rgb, sem_rgb], dim=-1) \n",
    "\n",
    "            T.ToPILImage()(combined).save(os.path.join(args.log_dir, \"images\", f\"step_{step}_frame_{random_frame_idx}.png\"))\n",
    "        \n",
    "        # TODO: args.log_dir\n",
    "        if args.local_rank == 0 and step > 0 and step % args.save_ckpt == 0:\n",
    "            ckpt_path = os.path.join(args.log_dir, \"ckpt\", f\"model_latest.pt\")\n",
    "            torch.save(model.module.state_dict(), ckpt_path)\n",
    "            print(f\"[Checkpoint] Saved model at step {step} to {ckpt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc510c5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "keyword argument repeated: scene_names (3462809542.py, line 29)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 29\u001b[0;36m\u001b[0m\n\u001b[0;31m    scene_names=[str(i).zfill(3) for i in range(300,600)],\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m keyword argument repeated: scene_names\n"
     ]
    }
   ],
   "source": [
    "# args includes: output_path, log_dir, save_image, save_ckpt, local_rank, max_epoch\n",
    "# args includes: image_dir, scene_names, sequence_length, batch_size\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--image_dir', type=str, required=True, help='Path to the input images')\n",
    "    parser.add_argument('--scene_names', type=str, nargs='+', required=True, help='Scene names, supports formats like 3 5 7 or (3,7)')\n",
    "    parser.add_argument('--sequence_length', type=int, default=4, help='Number of input frames')\n",
    "    parser.add_argument('--batch_size', type=int, default=1, help='Batch size for training')\n",
    "    parser.add_argument('--log_dir', type=str, required=True, help='Path to the log directory')\n",
    "    parser.add_argument('--save_image', type=int, default=10, help='Epoch intervals to save images')\n",
    "    parser.add_argument('--save_ckpt', type=int, default=10, help='Epoch intervals to save checkpoints')\n",
    "    parser.add_argument('--local_rank', type=int, default=0, help='Local rank for distributed training')\n",
    "    parser.add_argument('--max_epoch', type=int, default=10, help='Maximum number of epochs')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # dist.init_process_group(backend='nccl')\n",
    "    # args.local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "    device = torch.device(\"cuda\", args.local_rank)\n",
    "    dtype = torch.float32\n",
    "\n",
    "    # ================ Dataset ================\n",
    "\n",
    "    scene_names_str = ' '.join(args.scene_names)\n",
    "    scene_names = parse_scene_names(scene_names_str)\n",
    "\n",
    "    dataset = WaymoOpenDataset(\n",
    "        image_dir=args.image_dir,\n",
    "        scene_names=scene_names,\n",
    "        # scene_names=[str(i).zfill(3) for i in range(300,600)],\n",
    "        sequence_length=args.sequence_length,\n",
    "        mode=1,\n",
    "        views=1\n",
    "    )\n",
    "    sampler = DistributedSampler(dataset,shuffle=True)\n",
    "    dataloader = DataLoader(dataset, batch_size=args.batch_size, sampler=sampler, num_workers=4)\n",
    "\n",
    "    if args.local_rank == 0:\n",
    "        # os.makedirs(args.output_path, exist_ok=True)\n",
    "        os.makedirs(args.log_dir, exist_ok=True)\n",
    "        os.makedirs(os.path.join(args.log_dir, \"images\"), exist_ok=True)\n",
    "        os.makedirs(os.path.join(args.log_dir, \"ckpt\"), exist_ok=True)\n",
    "    # ================ Model ================\n",
    "\n",
    "    model = VGGT().to(device)\n",
    "    checkpoint = torch.load(args.ckpt_path, map_location=\"cpu\")\n",
    "    model.load_state_dict(checkpoint, strict=True)\n",
    "\n",
    "    model.train()\n",
    "    model = DDP(model, device_ids=[args.local_rank])\n",
    "    model._set_static_graph()\n",
    "    # PyTorch Lightning 会假设模型结构在前向传播中保持不变\n",
    "    # 避免在每个训练步骤中重新分析计算图，减少内存分配和释放的开销\n",
    "\n",
    "    loss_fn = FeedForwardLoss(lambda_o=1, lambda_d=0.05, lambda_l=0.01, lambda_lpips=0.05, device=device)\n",
    "\n",
    "    # for param in model.module.parameters():\n",
    "    #     param.requires_grad = False\n",
    "    # for head_name in [\"gs_head\",\"instance_head\",\"sky_model\" ]: #, \"gs_head\",\"instance_head\",\"sky_model\", \"semantic_head\"\n",
    "    #     for param in getattr(model.module, head_name).parameters():\n",
    "    #         param.requires_grad = True\n",
    "\n",
    "    # ================ Optimizer & Scheduler ================\n",
    "\n",
    "    optimizer = AdamW([\n",
    "        {'params': model.module.gs_head.parameters(), 'lr': 4e-5},\n",
    "        # {'params': model.module.semantic_head.parameters(), 'lr': 1e-4},\n",
    "        {'params': model.module.instance_head.parameters(), 'lr': 4e-5},\n",
    "        {'params': model.module.sky_model.parameters(), 'lr': 1e-4},\n",
    "    ], weight_decay=1e-4)\n",
    "\n",
    "    warmup_iterations = 1000\n",
    "    scheduler = LambdaLR(\n",
    "        optimizer,\n",
    "        lr_lambda=lambda step: min((step + 1) / warmup_iterations, 1.0) * 0.5 * (\n",
    "            1 + torch.cos(torch.tensor(torch.pi * step / args.max_epoch)))\n",
    "    )\n",
    "\n",
    "    for step in tqdm(range(args.max_epoch)):\n",
    "        train(model, dataloader, optimizer, scheduler, loss_fn, step, dtype, device, args)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dggt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
